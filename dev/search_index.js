var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = Jjama3","category":"page"},{"location":"#Jjama3","page":"Home","title":"Jjama3","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Jjama3.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [Jjama3]","category":"page"},{"location":"#Jjama3.generate-Union{Tuple{T}, Tuple{Transformer{T}, AbstractArray{<:Integer}}} where T","page":"Home","title":"Jjama3.generate","text":"generate(model, initial_tokens; max_new_tokens=100, sampler=top_pk_sampler(p=0.5f0, k=5), tokenizer_for_printing=tkn, end_token=128010)\n\nTakes an initial sequence of tokens, and generates new tokens one at a time until the end token is sampled. Uses a KV cache. No batch dim for now. Runs on CPU by default. If the model is on the GPU (assuming Flux.jl, eg. model = gpu(model)), then pass device = gpu to generate to run on the GPU.\n\ntkn = llama3_tokenizer()\ngenerate(model, initial_tokens; max_new_tokens=100, sampler=top_pk_sampler(p=0.5f0, k=5), tokenizer_for_printing=tkn, end_token=128010)\n\n\n\n\n\n","category":"method"},{"location":"#Jjama3.llama3_assistant_prompt-Tuple{Any, Any}","page":"Home","title":"Jjama3.llama3_assistant_prompt","text":"generate(model, prompt, max_new_tokens=100, encoder_for_printing=tkn)\n\nFormat a prompt for use with Llama3.2's instruction format, with a simple \"You are a helpful assistant\" system prompt.\n\nprompt = assistant_prompt(tkn, \"What is the capital of France?\")\ngenerate(model, prompt, max_new_tokens=100, encoder_for_printing=tkn)\n\n\n\n\n\n","category":"method"},{"location":"#Jjama3.load_llama3_from_safetensors-Tuple{Vector{String}, Any}","page":"Home","title":"Jjama3.load_llama3_from_safetensors","text":"model = load_llama3_from_safetensors(model_weight_paths, config)\n\nLoad a Llama3 model from a set of Huggingface safetensors files, and the config.json file. Important note: Huggingface uses a different RoPE convention than other implementations, so if you're loading weights from a different source, you might get very poor model performance.\n\nusing JSON3\nconfig = JSON3.read(read(\"Llama3_2_1B_instruct/config.json\", String))\nmodel_weight_paths = [\"Llama3_2_1B_instruct/model.safetensors\"] #Can be an array of paths if the model is split across multiple files\nmodel = load_llama3_from_safetensors(model_weight_paths, config)\n\n\n\n\n\n","category":"method"},{"location":"#Jjama3.structured_choice-Tuple{Vector{String}, Vector{String}, Int64}","page":"Home","title":"Jjama3.structured_choice","text":"sampler = structured_choice(choices, vocab::Vector{String}, end_token::Int; sampler = logits -> argmax_sampler(logits))\n\nReturn a function that can be passed into generate as a sampler, which will sample from the given choices. Handles the case where the choices are made up of multiple tokens. vocab is an array of the tokens as strings, in their order in the tokenizer. sampler is a function that takes the logits (here including those masked with -Inf) and returns a sample from them. Defaults to argmax.\n\nExample:\n\nconfig = JSON3.read(read(\"SmolLM2-1.7B-Instruct/config.json\", String))\nmodel = load_llama3_from_safetensors(\"SmolLM2-1.7B-Instruct/model.safetensors\", config)\ntkn = tokenizer_from_file(Tokenizer, \"SmolLM2-1.7B-Instruct/tokenizer.json\")\n\nquestion = \"In a Bayesian model, what do we call the probability distribution of parameters given the data?\"\nchoices = [\"Prior\", \"Likelihood\", \"Marginal Likelihood\", \"Evidence\", \"Posterior\"]\n\nvocab = [decode(tkn, [i], skip_special_tokens = false) for i in 1:49152]\neos = encode(tkn, \"<|im_end|>\")[end]\nprompt = smollm2_instruct_prompt(tkn, \"You are an expert in Statistics and Probability Theory who answers questions in as few words as possible.\",question)\ngenerate(model, prompt, max_new_tokens=100, tokenizer_for_printing=tkn, end_token = eos, sampler = structured_choice(choices, vocab, eos));\n\nIf you want to run the model on the GPU, then you need to pass device = gpu to the generate function, and device = cpu to the structured_choice function.\n\n\n\n\n\n","category":"method"}]
}
